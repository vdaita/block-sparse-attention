{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I6Z3MHRU5TVV"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Make sure your runtime has a GPU enabled.\n",
    "'''\n",
    "# For building CUDA\n",
    "!pip install Ninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NfZ_GHylbyew"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.cpp_extension import load_inline\n",
    "\n",
    "cuda_src = '''\n",
    "template<int BLOCK_SIZE, int D>\n",
    "__global__\n",
    "void forward_kernel(\n",
    "    const float* Q,\n",
    "    const float* K,\n",
    "    const float* V,\n",
    "    const int* block_indices,\n",
    "    const int Nq,\n",
    "    const int Nk, \n",
    "    const int B,\n",
    "    const int block_count,\n",
    "    const float softmax_scale,\n",
    "    float* output,\n",
    "    const bool use_causal_mask\n",
    ") {\n",
    "    int tx = threadIdx.x; // Specific query being processed\n",
    "    int blockSize = blockDim.x; // This specifies how big the given block is\n",
    "    int bx = blockIdx.x; // Specific block being processed\n",
    "    int by = blockIdx.y; // Specifies the batch/head currently being processed. The input to this kernel should be reshaped such that the first dimension is B * H\n",
    "\n",
    "    int q_offset = by * Nq * D + bx * blockSize * D + tx * D;\n",
    "\n",
    "    extern __shared__ float shared_memory[];\n",
    "    int tile_size = blockSize * D;\n",
    "    float* shared_q = shared_memory;\n",
    "    float* shared_k = &shared_memory[tile_size];\n",
    "    float* shared_v = &shared_memory[2 * tile_size];\n",
    "\n",
    "    float running_max = -INFINITY;\n",
    "    float running_sum = 0;\n",
    "\n",
    "    float* acc[BLOCK_SIZE][D] = {0};\n",
    "    float* P[BLOCK_SIZE] = {0};\n",
    "\n",
    "    for(int j = 0; j < block_count; j++){\n",
    "        for(int x = 0; x < D; x++){ // should try to introduce coalescing into this loop\n",
    "            shared_k[tx * D + x] = K[by * Nk * D + block_indices[j] * blockSize * D + tx * D + x];\n",
    "            shared_v[tx * D + x] = V[by * Nk * D + block_indices[j] * blockSize * D + tx * D + x];\n",
    "        }\n",
    "        __syncthreads();\n",
    "\n",
    "        float new_max = running_max;\n",
    "\n",
    "        for(int i = 0; i < blockSize; i++){\n",
    "            float dot_product = 0;\n",
    "            for(int x = 0; x < D; x++){\n",
    "                dot_product += Q[q_offset + x] * shared_k[i * D + x];\n",
    "            }\n",
    "            dot_product *= softmax_scale;\n",
    "            if(use_causal_mask){\n",
    "                if(block_indices[j] * blockSize + i > bx * blockSize + tx){\n",
    "                    dot_product = -INFINITY;\n",
    "                }\n",
    "            }\n",
    "            P[i] = dot_product;\n",
    "            new_max = fmaxf(new_max, dot_product);\n",
    "        }\n",
    "\n",
    "        float alpha = __exp2(new_max - running_max);\n",
    "        running_sum *= (1 / alpha);\n",
    "        for(int i = 0; i < blockSize; i++){\n",
    "            float token_weight = __exp2(P[i] - new_max);\n",
    "            running_sum += token_weight;\n",
    "            for(int x = 0; x < D; x++){\n",
    "                acc[i][x] *= (1 / alpha);\n",
    "                acc[i][x] += token_weight * shared_v[i * D + x];\n",
    "            }\n",
    "        }\n",
    "\n",
    "        running_max = new_max;\n",
    "        __syncthreads();\n",
    "    }\n",
    "\n",
    "    for(int i = 0; i < blockSize; i++){\n",
    "        for(int x = 0; x < D; x++){\n",
    "            output[by * Nq * D + bx * blockSize * D + i * D + x] = acc[i][x] / running_sum;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "torch::Tensor forward(\n",
    "    torch::Tensor queries,\n",
    "    torch::Tensor keys,\n",
    "    torch::Tensor values,\n",
    "    torch::Tensor query_blocks,\n",
    "    int64_t block_size,\n",
    "    float dropout_p) {\n",
    "    \n",
    "    // the first dimensions should be B, H, T, D\n",
    "    int B = queries.size(0);\n",
    "    int H = queries.size(1);\n",
    "    int T = queries.size(2);\n",
    "    int D = queries.size(3);\n",
    "\n",
    "    dim3 gridDim(ceil(T * 1.0f / block_size), B * H, 1);\n",
    "    dim3 blockDim(block_size, 1, 1);\n",
    "\n",
    "    auto output = torch::zeros_like(queries);\n",
    "\n",
    "    forward_kernel<block_size, D><<<gridDim, blockDim>>>(\n",
    "        queries.data_ptr<float>(),\n",
    "        keys.data_ptr<float>(),\n",
    "        values.data_ptr<float>(),\n",
    "        query_blocks.data_ptr<int>(),\n",
    "        T,\n",
    "        T,\n",
    "        B * H,\n",
    "        query_blocks.size(0),\n",
    "        1.0 / sqrtf(D),\n",
    "        output.data_ptr<float>(),\n",
    "        false\n",
    "    );\n",
    "\n",
    "    return output;\n",
    "}\n",
    "'''\n",
    "cpp_src = 'torch::Tensor forward(torch::Tensor Q, torch::Tensor K, torch::Tensor V, torch::Tensor query_blocks, int64_t block_size);'\n",
    "\n",
    "build_dir = 'cuda'\n",
    "if not os.path.exists(build_dir):\n",
    "    os.mkdir(build_dir)\n",
    "\n",
    "block_sparse_attention = load_inline(\n",
    "    name='block_sparse_attention',\n",
    "    cpp_sources=cpp_src,\n",
    "    cuda_sources=cuda_src,\n",
    "    functions=['forward'],\n",
    "    with_cuda=True,\n",
    "    extra_cuda_cflags=['-O2'],\n",
    "    build_directory=f'./{build_dir}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z8mHyg-X4lb0"
   },
   "outputs": [],
   "source": [
    "B = 2\n",
    "H = 4\n",
    "T = 100\n",
    "D = 128\n",
    "block_size = 16\n",
    "\n",
    "q = torch.randn(B, H, T, D).cuda()\n",
    "k = torch.randn(B, H, T, D).cuda()\n",
    "v = torch.randn(B, H, T, D).cuda()\n",
    "block_indices = torch.randint(0, T // block_size, (B, H, T // block_size, 4)).cuda()\n",
    "\n",
    "print('=== profiling manual attention ===')\n",
    "\n",
    "# Our minimal flash attention needs to be faster than this.\n",
    "def baseline_block_sparse_attention(q, k, v, block_indices, block_size):\n",
    "    B, H, T, D = q.shape\n",
    "    O = torch.zeros_like(v)\n",
    "\n",
    "    for b in range(B):\n",
    "        for h in range(H):\n",
    "            bh_output = []\n",
    "            for query_block_index in range((T + block_size - 1) // block_size):\n",
    "                query_block = q[b, h, query_block_index * block_size : (query_block_index + 1) * block_size, ...]\n",
    "                key_blocks = []\n",
    "                value_blocks = []\n",
    "                for block_indices in block_indices[b][h][query_block_index]:\n",
    "                    key_block = k[b, h, block_indices * block_size : (block_indices + 1) * block_size, ...]\n",
    "                    key_blocks.append(key_block)\n",
    "\n",
    "                    value_block = v[b, h, block_indices * block_size : (block_indices + 1) * block_size, ...]\n",
    "                    value_blocks.append(value_block)\n",
    "                key_block = torch.cat(key_blocks, dim=0)\n",
    "                value_block = torch.cat(value_blocks, dim=0)\n",
    "\n",
    "                attention = torch.matmul(query_block, key_block.transpose(-2, -1))\n",
    "                attention = attention / (D ** 0.5)\n",
    "                attention = torch.nn.functional.softmax(attention, dim=-1)\n",
    "                output = torch.matmul(attention, value_block)\n",
    "                bh_output.append(output)\n",
    "            bh_output = torch.cat(bh_output, dim=0)\n",
    "            O[b, h, ...] = bh_output\n",
    "    return O\n",
    "\n",
    "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "    manual_result = baseline_block_sparse_attention(q, k, v, block_indices, block_size)\n",
    "print(prof.key_averages().table(sort_by='cuda_time_total', row_limit=10))\n",
    "\n",
    "print('=== profiling custom cuda block flash attention === ')\n",
    "\n",
    "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "    minimal_result = block_sparse_attention.forward(q, k, v)\n",
    "print(prof.key_averages().table(sort_by='cuda_time_total', row_limit=10))\n",
    "\n",
    "print('attn values sanity check:', torch.allclose(minimal_result, manual_result, rtol=0, atol=1e-02))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
